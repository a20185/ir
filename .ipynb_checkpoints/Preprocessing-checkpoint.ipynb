{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "# 基本部件的引用声明\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "import shutil\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import graphlab\n",
    "import numpy as np\n",
    "from array import array\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意我们这里需要安装好的依赖有Graphlab，numpy，jieba\n",
    "* Numpy和Jieba直接pip即可\n",
    "* Graphlab是商业软件，需要授权\n",
    "    * PIP安装时用此命令：(括号内可选)\n",
    "         * (sudo) pip install (--user) --upgrade --no-cache-dir https://get.graphlab.com/GraphLab-Create/2.0.1/ouyf5@mail2.sysu.edu.cn/F822-FCB4-973D-5C49-44E0-430C-BC81-4EA3/GraphLab-Create-License.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先是从文件中的读取以及分词\n",
    "\n",
    "\n",
    "此处用的是 *Jieba* 中文分词器，在这里我们将中文文本分词之后以空格分隔各词然后输出到目标文件夹\n",
    "\n",
    "函数定义中需传入 **CSV文件的绝对地址** \n",
    "\n",
    "格式要求：\n",
    "   * CSV中字串以UTF-8格式编码，CSV文件格式为id,content(需要有Header Row)\n",
    "   * 传入目标文件为绝对地址\n",
    "   * 获得的数据为SFrame格式，column与CSV格式一致\n",
    "   * SFrame参考文档：https://turi.com/products/create/docs/generated/graphlab.SFrame.html?highlight=sframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首先是添加停词表（Stop Words List）\n",
    "* 加入停词表能够把常用的词语（通常是介词、连接词等）去掉，提高分类的准确性\n",
    "* get_stop_words 这个方法从文件中读取停词列表，然后返回一个 **停词集合**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_file(path):\n",
    "    with open(path,\"r\") as fp:\n",
    "        words = fp.read()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stop_words(filepath):\n",
    "    words = read_from_file(filepath)\n",
    "    result = jieba.cut(words)\n",
    "    new_words = []\n",
    "    for r in result:\n",
    "        new_words.append(r)\n",
    "    return set(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(words,stop_words_set):\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if w not in stop_words_set:\n",
    "            new_words.append(w)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint: 检查上述方法的正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_and_cut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e861df7afb14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#    print lister[i]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mweibo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#print weibo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_and_cut' is not defined"
     ]
    }
   ],
   "source": [
    "#stop_set = get_stop_words('./stop.txt')\n",
    "#lister = list(stop_set)\n",
    "#for i in range(10):\n",
    "#    print lister[i]\n",
    "\n",
    "weibo = read_and_cut('./data.csv')\n",
    "#print weibo\n",
    "\n",
    "train_fake_news = weibo[0:10]\n",
    "train_fake_weibos = weibo[10:]\n",
    "\n",
    "weibos_wvec = batch_word_vec_generator(train_fake_weibos['parsed'] , True)\n",
    "news_wvec = batch_word_vec_generator(train_fake_news['parsed'] , True)\n",
    "\n",
    "\n",
    "train_fake_news['word_vec'] = news_wvec['word_vec']\n",
    "train_fake_news['tf_word_vec'] = news_wvec['tf_word_vec']\n",
    "train_fake_news['assign_weibos'] = batch_NN_finder(train_fake_news['tf_word_vec'] , weibos_wvec['tf_word_vec'] , train_fake_weibos['Id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是正式操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_cut(csvpath):\n",
    "    mydata = graphlab.SFrame(csvpath)\n",
    "    stop_set = get_stop_words('./stop.txt')\n",
    "    parsed = []\n",
    "    linen = {0,1,2,3,4,5,6,7,8,9,19,39,59,99,399,599,999,2999,3999,5999,8888,9999}\n",
    "    \n",
    "    for i in range(len(mydata)):\n",
    "        line = unicode(mydata['text'][i] , \"utf-8\")\n",
    "        seglist = jieba.cut(line , cut_all = False)\n",
    "        parsed_seg = remove_stop_words(seglist , stop_set)\n",
    "        output = ' '.join(parsed_seg)\n",
    "        parsed.append(output)\n",
    "        if (i in linen or i == len(mydata) - 1):\n",
    "            print \"处理 #%d 文章完成\" % i\n",
    "    mydata['parsed'] = parsed\n",
    "    print \"全部完成！\"\n",
    "    return mydata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来这个函数是用来提取*词数向量*的\n",
    "\n",
    "* 在分析过程中，可以频繁的调用该函数，需要提供 **原始文本的字串**\n",
    "* 返回值为一个 *Dict<单词，出现次数>* ,\n",
    "* 返回的Dict没有依据单词出现的次数排序\n",
    "* 如果用向量实现并不现实，所以在此处我们使用Dict来实现词数向量的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_word_vec(ref_str):\n",
    "    words = [w.strip() for w in ref_str.split() if w.strip()]\n",
    "    counter = Counter()\n",
    "    for w in words:\n",
    "        counter[w] += 1\n",
    "    # kv = counter.items()   #这是键值对的列表\n",
    "    wc_dict = dict(counter)    #这是词典\n",
    "    #可以用列表做其他事，但只返回词典\n",
    "    return wc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来是计算两个词数向量间的距离\n",
    "\n",
    "* euc_dist 计算两个词数向量之间的欧几里得距离 返回值为浮点数\n",
    "* cos_dist 计算两个词数向量之间的余弦距离 返回值为浮点数\n",
    "* jac_dist 计算两个词数向量之间的加权杰卡德距离 返回值为浮点数\n",
    "\n",
    "注意到，此处只是使用计算距离对词数向量进行Brute-Force计算，很容易把一些常见的词错误的认为是判断两个文本文档相类似的函数的主要依据（如 **\"的\"** , **\"和\"** , **\"是\"**  , **\"了\"**  , etc）。如果需要计算更精确的数值，应该使用 **TF-IDF** 来进行计算,使用者在使用前应该调用 *calc_tf_idf* 方法。总体来说，即便使用TFIDF处理过，中文文本的聚类正确率还是不是非常的理想。所以可以尽情尝试各种距离（经过初步实验，Euclidean Distance准确度稍微高那么一点点。）\n",
    "\n",
    "* 注意, *calc_tf_idf* 方法需要使用者传入所有文本（whole documents）的 **SArray词典集合（SArray of dict）** 返回值为SArray的词典集合类型（每行分别为原来单词的dict的SArray集合）。(SArray文档：https://turi.com/products/create/docs/generated/graphlab.SArray.html?highlight=sarray)\n",
    "* 示例使用方法：\n",
    "    ```python\n",
    "        docs['TF_IDF'] = calc_tf_idf(docs['wc_dict'])\n",
    "    ```\n",
    "    这样处理之后还是可以通过\n",
    "    ```python\n",
    "    docs[docs['dName'] == 'docnamehere'][0]\n",
    "    ```\n",
    "    找到某行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def euc_dist(dict_a , dict_b):\n",
    "    return graphlab.distances.euclidean(dict_a , dict_b)\n",
    "\n",
    "def cos_dist(dict_a , dict_b):\n",
    "    return graphlab.distances.cosine(dict_a , dict_b)\n",
    "\n",
    "def jac_dist(dict_a , dict_b):\n",
    "    return graphlab.distances.weighted_jaccard(dict_a , dict_b)\n",
    "\n",
    "def set_calculation(dict_a , dict_b):\n",
    "    seta = set(dict_a.keys())\n",
    "    setb = set(dict_b.keys())\n",
    "    inter = seta.intersection(setb)\n",
    "    result = 0\n",
    "    for val in inter:\n",
    "        result = result + dict_a[val] * dict_b[val]\n",
    "    return result\n",
    "\n",
    "# set similarity: Larger is better\n",
    "def set_dist(dict_small , dict_large , lg_whole_orig , lg_whole_vec):\n",
    "    result = 0\n",
    "    for k in dict_small:\n",
    "        if k in dict_large and dict_small[k] > 0:\n",
    "            # threshold: 0.1\n",
    "            if lg_whole_vec[k] <= 0.1 and dict_large[k] > 0:\n",
    "                result = lg_whole_orig[k] * dict_small[k] + result\n",
    "            elif dict_large[k] <= 0:\n",
    "                result = result + 0\n",
    "            else:\n",
    "                result = lg_whole_vec[k] * dict_small[k] + result\n",
    "        else:\n",
    "            result  = result + 0\n",
    "    return result\n",
    "\n",
    "def calc_tf_idf(whole_document_dict):\n",
    "    return graphlab.text_analytics.tf_idf(whole_document_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whole_wc_vec(whole_dict):\n",
    "    wholes = {}\n",
    "    for i in range(len(whole_dict)):\n",
    "        test = whole_dict[i]\n",
    "        for k in test:\n",
    "            if k not in wholes:\n",
    "                wholes[k] = test[k]\n",
    "            else:\n",
    "                wholes[k] = wholes[k] + test[k]\n",
    "    return wholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 给某一个特定的新闻找到10个左右Nearnest Neighbour\n",
    "\n",
    "由于我们在数据库中存放的是微博，所以我们简单的给定 *新闻文档* ，然后给其适配最适合的10个微博。\n",
    "* 距离此处使用了欧式距离。\n",
    "* 初始阈值设定为50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 不管传入的是否是TF-IDF，要求传的新闻词数向量为Dictionary，whole_document_vec为dict的SFrame\n",
    "def find_NN(news_vec , whole_document_vec , doc_ids):\n",
    "    process_vec = graphlab.SFrame()\n",
    "    #process_vec['original'] = whole_document_vec\n",
    "    result_value = []\n",
    "    result_id = []\n",
    "\n",
    "    for i in range(len(whole_document_vec)):\n",
    "        cos_val = set_calculation(whole_document_vec[i] , news_vec)\n",
    "        if cos_val <= 10000:\n",
    "            result_id.append(doc_ids[i])\n",
    "            result_value.append(cos_val)\n",
    "    process_vec['similarity'] = result_value\n",
    "    process_vec['id'] = result_id\n",
    "\n",
    "    my = process_vec.sort('similarity' , ascending=False)\n",
    "\n",
    "    if len(process_vec) < 10:\n",
    "        return my\n",
    "    else:\n",
    "        return my[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量生成工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_word_vec_generator(dict_set , TF_SELECTOR = False):\n",
    "        gen_result = graphlab.SFrame()\n",
    "        word_vec = []\n",
    "        linen = {0,1,2,3,4,5,6,7,8,9,19,39,59,99,399,599,999,2999,3999,5999,8888,9999}\n",
    "        for i in range(len(dict_set)):\n",
    "                temp_vec = parse_word_vec(dict_set[i])\n",
    "                word_vec.append(temp_vec)\n",
    "                if i in linen or i == len(dict_set) - 1:\n",
    "                    print \"第 %d 条文章原始词向量计算完成\" % i\n",
    "        gen_result['word_vec'] = word_vec\n",
    "        print \"本数据集的原始词向量全部计算完成\"\n",
    "        if TF_SELECTOR:\n",
    "                gen_result['tf_word_vec'] = calc_tf_idf(gen_result['word_vec'])\n",
    "                print \"本数据集的TF-IDF词向量全部计算完成\"\n",
    "        return gen_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_NN_finder(news , weibos , weibo_ids):\n",
    "    linen = {0,1,2,3,4,5,6,7,8,9,19,39,59,99,199,399,599,799,999,1299,1499,1699,1999,2499,2999,3499,3999,4499,4999,5999,8888,9999}\n",
    "    assignation = []\n",
    "    for i in range(len(news)):\n",
    "        temp_nns = find_NN(news[i] , weibos , weibo_ids)\n",
    "        assignation.append(temp_nns)\n",
    "        if i in linen or i == len(news) - 1:\n",
    "            print \"第 %d 条新闻的最近邻居计算完成\" % (i + 1)\n",
    "    return assignation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数测试完成，下面是预处理总方法\n",
    "\n",
    "由于我们需要的是新闻AND ITS 匹配微博\n",
    "所以本方法只返回一个SFrame，列为 <*新闻ID* , *新闻原文* , *新闻原始词向量* ,*tf词向量* ,*匹配微博* >\n",
    "\n",
    "注意：**匹配微博**为一个对象，其中含有匹配微博的< *id* , *相似度*>\n",
    "\n",
    "\n",
    "接受的参数为：\n",
    "* weibo_src: 微博CSV文件的绝对路径\n",
    "* news_src: 新闻CSV文件的绝对路径\n",
    "* TF_SELECTOR: TF-IDF选择子，表示是否需要进行TF词向量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Main process\n",
    "#Result contains a SFrame that contains news raw word_vec and matching weibo\n",
    "def pre_process(weibo_src , news_src , TF_SELECTOR=False):\n",
    "    #分别处理微博和新闻\n",
    "    weibos = read_and_cut(weibo_src)\n",
    "    news = read_and_cut(news_src)\n",
    "    \n",
    "    #计算词向量和TF词向量\n",
    "    weibos_wvec = batch_word_vec_generator(weibos['parsed'] , TF_SELECTOR)\n",
    "    news_wvec = batch_word_vec_generator(news['parsed'] , TF_SELECTOR)\n",
    "    \n",
    "    news['word_vec'] = news_wvec['word_vec']\n",
    "    \n",
    "    if TF_SELECTOR:\n",
    "        news['tf_word_vec'] = news_wvec['tf_word_vec']\n",
    "        news['assign_weibos'] = batch_NN_finder(news['tf_word_vec'] , weibos_wvec['tf_word_vec'] , weibos['id'])\n",
    "    else:\n",
    "        news['assign_weibos'] = batch_NN_finder(news['word_vec'] , weibos_wvec['word_vec'] , weibos['id'])\n",
    "\n",
    "    return news,weibos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create is assigned to ouyf5@mail2.sysu.edu.cn and will expire on November 25, 2016. For commercial licensing options, visit https://turi.com/buy/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-07-21 17:45:48,424 [INFO] graphlab.cython.cy_server, 176: GraphLab Create v1.8.5 started. Logging: /tmp/graphlab_server_1469094345.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5JkIGf\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5JkIGf\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5lVabK\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5lVabK\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55WxAd\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55WxAd\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55YTvx\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55YTvx\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"你的委屈要自己消化 你的故事不用逢人就讲起\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"你的委屈要自己消化 你的故事不用逢人就讲起\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"真正理解你的没有几个\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"真正理解你的没有几个\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55uIb1\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55uIb1\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"大多人只会站在他们自己的立场 偷看你的笑话\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"大多人只会站在他们自己的立场 偷看你的笑话\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"你能做的就是 把秘密藏起来 然后一步一步变得越来越强大”（via思想聚焦）\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"你能做的就是 把秘密藏起来 然后一步一步变得越来越强大”（via思想聚焦）\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5j1pM0\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5j1pM0\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>52 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "52 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/weibo.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/weibo.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.053062 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.053062 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5JkIGf\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5JkIGf\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55WxAd\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55WxAd\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55YTvx\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55YTvx\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55uIb1\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55uIb1\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5tE6jp\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5tE6jp\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5lVabK\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5lVabK\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"周到上海致力于为上海市民打造一个生活服务指南平台，让你足不出户知晓申城大小事，动动手指解决身边烦心事。\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"周到上海致力于为上海市民打造一个生活服务指南平台，让你足不出户知晓申城大小事，动动手指解决身边烦心事。\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"——路遥《平凡的世界》\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"——路遥《平凡的世界》\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"你的委屈要自己消化 你的故事不用逢人就讲起\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"你的委屈要自己消化 你的故事不用逢人就讲起\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"真正理解你的没有几个\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"真正理解你的没有几个\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>52 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "52 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-07-21 17:45:48,610 [DEBUG] jieba, 111: Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2016-07-21 17:45:48,886 [DEBUG] jieba, 131: Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/weibo.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/weibo.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.523 seconds.\n",
      "2016-07-21 17:45:49,409 [DEBUG] jieba, 163: Loading model cost 0.523 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 4379 lines in 0.055248 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 4379 lines in 0.055248 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n",
      "2016-07-21 17:45:49,414 [DEBUG] jieba, 164: Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理 #0 文章完成\n",
      "处理 #1 文章完成\n",
      "处理 #2 文章完成\n",
      "处理 #3 文章完成\n",
      "处理 #4 文章完成\n",
      "处理 #5 文章完成\n",
      "处理 #6 文章完成\n",
      "处理 #7 文章完成\n",
      "处理 #8 文章完成\n",
      "处理 #9 文章完成\n",
      "处理 #19 文章完成\n",
      "处理 #39 文章完成\n",
      "处理 #59 文章完成\n",
      "处理 #99 文章完成\n",
      "处理 #399 文章完成\n",
      "处理 #599 文章完成\n",
      "处理 #999 文章完成\n",
      "处理 #2999 文章完成\n",
      "处理 #3999 文章完成\n",
      "处理 #4378 文章完成\n",
      "全部完成！\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t作者 宋宁\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t作者 宋宁\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t　2016-06-12 11:25\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t　2016-06-12 11:25\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t交汇点讯 6月12日早上7点多，南京雨下个不停，在江宁区东山街道大里聚福城的康居园1栋，有人发现一名老太坠楼，大家赶 紧报警。警察法医赶到现场，发现老太已经死亡。老太坠楼地点位于...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t交汇点讯 6月12日早上7点多，南京雨下个不停，在江宁区东山街道大里聚福城的康居园1栋，有人发现一名老太坠楼，大家赶 紧报警。警察法医赶到现场，发现老太已经死亡。老太坠楼地点位于...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t“有意见，你去咬狗一口啊”■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t“有意见，你去咬狗一口啊”■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        粤公网安备 44010402000035号\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        粤公网安备 44010402000035号\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>183 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "183 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/news.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/news.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.086001 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.086001 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t作者 宋宁\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t作者 宋宁\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t　2016-06-12 11:25\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t　2016-06-12 11:25\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t交汇点讯 6月12日早上7点多，南京雨下个不停，在江宁区东山街道大里聚福城的康居园1栋，有人发现一名老太坠楼，大家赶 紧报警。警察法医赶到现场，发现老太已经死亡。老太坠楼地点位于...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t交汇点讯 6月12日早上7点多，南京雨下个不停，在江宁区东山街道大里聚福城的康居园1栋，有人发现一名老太坠楼，大家赶 紧报警。警察法医赶到现场，发现老太已经死亡。老太坠楼地点位于...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t“有意见，你去咬狗一口啊”■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t“有意见，你去咬狗一口啊”■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"渝ICP备12004790号-1 公安部门备案编号：5000085100200005  ©2015上游\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"渝ICP备12004790号-1 公安部门备案编号：5000085100200005  ©2015上游\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>183 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "183 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/news.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/news.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1493 lines in 0.12073 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1493 lines in 0.12073 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理 #0 文章完成\n",
      "处理 #1 文章完成\n",
      "处理 #2 文章完成\n",
      "处理 #3 文章完成\n",
      "处理 #4 文章完成\n",
      "处理 #5 文章完成\n",
      "处理 #6 文章完成\n",
      "处理 #7 文章完成\n",
      "处理 #8 文章完成\n",
      "处理 #9 文章完成\n",
      "处理 #19 文章完成\n",
      "处理 #39 文章完成\n",
      "处理 #59 文章完成\n",
      "处理 #99 文章完成\n",
      "处理 #399 文章完成\n",
      "处理 #599 文章完成\n",
      "处理 #999 文章完成\n",
      "处理 #1492 文章完成\n",
      "全部完成！\n"
     ]
    }
   ],
   "source": [
    "weibopath = './weibo.csv'\n",
    "newspath = './news.csv'\n",
    "TF_SELECTOR = True\n",
    "\n",
    "weibos = read_and_cut(weibopath)\n",
    "news = read_and_cut(newspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算微博词向量\n",
      "第 0 条文章原始词向量计算完成\n",
      "第 1 条文章原始词向量计算完成\n",
      "第 2 条文章原始词向量计算完成\n",
      "第 3 条文章原始词向量计算完成\n",
      "第 4 条文章原始词向量计算完成\n",
      "第 5 条文章原始词向量计算完成\n",
      "第 6 条文章原始词向量计算完成\n",
      "第 7 条文章原始词向量计算完成\n",
      "第 8 条文章原始词向量计算完成\n",
      "第 9 条文章原始词向量计算完成\n",
      "第 19 条文章原始词向量计算完成\n",
      "第 39 条文章原始词向量计算完成\n",
      "第 59 条文章原始词向量计算完成\n",
      "第 99 条文章原始词向量计算完成\n",
      "第 399 条文章原始词向量计算完成\n",
      "第 599 条文章原始词向量计算完成\n",
      "第 999 条文章原始词向量计算完成\n",
      "第 2999 条文章原始词向量计算完成\n",
      "第 3999 条文章原始词向量计算完成\n",
      "第 4378 条文章原始词向量计算完成\n",
      "本数据集的原始词向量全部计算完成\n",
      "本数据集的TF-IDF词向量全部计算完成\n",
      "微博词向量全部计算完成\n",
      "\n",
      "开始计算新闻词向量\n",
      "第 0 条文章原始词向量计算完成\n",
      "第 1 条文章原始词向量计算完成\n",
      "第 2 条文章原始词向量计算完成\n",
      "第 3 条文章原始词向量计算完成\n",
      "第 4 条文章原始词向量计算完成\n",
      "第 5 条文章原始词向量计算完成\n",
      "第 6 条文章原始词向量计算完成\n",
      "第 7 条文章原始词向量计算完成\n",
      "第 8 条文章原始词向量计算完成\n",
      "第 9 条文章原始词向量计算完成\n",
      "第 19 条文章原始词向量计算完成\n",
      "第 39 条文章原始词向量计算完成\n",
      "第 59 条文章原始词向量计算完成\n",
      "第 99 条文章原始词向量计算完成\n",
      "第 399 条文章原始词向量计算完成\n",
      "第 599 条文章原始词向量计算完成\n",
      "第 999 条文章原始词向量计算完成\n",
      "第 1492 条文章原始词向量计算完成\n",
      "本数据集的原始词向量全部计算完成\n",
      "本数据集的TF-IDF词向量全部计算完成\n",
      "新闻词向量全部计算完成\n"
     ]
    }
   ],
   "source": [
    "print \"开始计算微博词向量\"\n",
    "weibos_wvec = batch_word_vec_generator(weibos['parsed'] , TF_SELECTOR)\n",
    "print \"微博词向量全部计算完成\"\n",
    "print \"\"\n",
    "print \"开始计算新闻词向量\"\n",
    "news_wvec = batch_word_vec_generator(news['parsed'] , TF_SELECTOR)\n",
    "print \"新闻词向量全部计算完成\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算最近邻居\n",
      "第 1 条新闻的最近邻居计算完成\n",
      "第 2 条新闻的最近邻居计算完成\n",
      "第 3 条新闻的最近邻居计算完成\n",
      "第 4 条新闻的最近邻居计算完成\n",
      "第 5 条新闻的最近邻居计算完成\n",
      "第 6 条新闻的最近邻居计算完成\n",
      "第 7 条新闻的最近邻居计算完成\n",
      "第 8 条新闻的最近邻居计算完成\n",
      "第 9 条新闻的最近邻居计算完成\n",
      "第 10 条新闻的最近邻居计算完成\n",
      "第 20 条新闻的最近邻居计算完成\n",
      "第 40 条新闻的最近邻居计算完成\n",
      "第 60 条新闻的最近邻居计算完成\n",
      "第 100 条新闻的最近邻居计算完成\n",
      "第 200 条新闻的最近邻居计算完成\n",
      "第 400 条新闻的最近邻居计算完成\n",
      "第 600 条新闻的最近邻居计算完成\n",
      "第 800 条新闻的最近邻居计算完成\n",
      "第 1000 条新闻的最近邻居计算完成\n",
      "第 1300 条新闻的最近邻居计算完成\n",
      "第 1493 条新闻的最近邻居计算完成\n",
      "最近邻居全部计算完成\n"
     ]
    }
   ],
   "source": [
    "news['word_vec'] = news_wvec['word_vec']\n",
    "news['tf_word_vec'] = news_wvec['tf_word_vec']\n",
    "print \"开始计算最近邻居\"\n",
    "\n",
    "\n",
    "news['assign_weibos'] = batch_NN_finder(news['tf_word_vec'] , weibos_wvec['tf_word_vec'] , weibos['id'])\n",
    "print \"最近邻居全部计算完成\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news.save('./news_finished')\n",
    "weibos.save('./weibo_parsed')\n",
    "weibos_wvec.save('./weibo_wordvec')\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(test)):\n",
    "#     print \"\"\n",
    "#     print \"+++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "#     print \"-------------------------------------------------------\"\n",
    "#     print \"分析新闻编号： %s.\" % test['id'][i]\n",
    "#     print \"该新闻内容如下：\"\n",
    "#     print test['text'][i]\n",
    "#     print \"\"\n",
    "#     print \"总共匹配文章数： %d.\" % len(test['assign_weibos'][i])\n",
    "#     print \"以下是匹配列表：\"\n",
    "#     print \"-------------------------------------------------------\"\n",
    "#     for ii in range(len(test['assign_weibos'][i])):\n",
    "#         print \"--------------------------------------------------------\"\n",
    "#         print \"匹配微博： #%d.\" % test['assign_weibos'][i][ii]['id']\n",
    "#         print weibos[weibos['id'] == test['assign_weibos'][i][ii]['id']]['text']\n",
    "#         print \"计算所得距离系数： %f.\" % test['assign_weibos'][i][ii]['similarity']\n",
    "#         print \"--------------------------------------------------------\"\n",
    "#     print \"--------------------------------------------------------\"\n",
    "#     print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "#     print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算微博总词向量\n",
      "开始计算新闻总词向量\n"
     ]
    }
   ],
   "source": [
    "# print \"开始计算微博总词向量\"\n",
    "# whole_weibos_dict = whole_wc_vec(weibos_wvec['tf_word_vec'])\n",
    "# print \"开始计算新闻总词向量\"\n",
    "# whole_news_dict = whole_wc_vec(news['tf_word_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846.25709595\n",
      "4093.24159478\n"
     ]
    }
   ],
   "source": [
    "# #len(whole_weibos_dict)\n",
    "# x = whole_weibos_dict.values()\n",
    "# x.sort(reverse=True)\n",
    "# print x[0]\n",
    "\n",
    "# y = whole_news_dict.values()\n",
    "# y.sort(reverse=True)\n",
    "# print y[0]\n",
    "\n",
    "# weibo_reg = dict(whole_weibos_dict)\n",
    "# for k in weibo_reg:\n",
    "#     weibo_reg[k] = weibo_reg[k] / x[0]\n",
    "\n",
    "# news_reg = dict(whole_news_dict)\n",
    "# for k in news_reg:\n",
    "#     news_reg[k] = news_reg[k] / y[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
