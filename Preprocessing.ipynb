{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "# 基本部件的引用声明\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "import os\n",
    "import shutil\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "import graphlab\n",
    "import numpy as np\n",
    "from array import array\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意我们这里需要安装好的依赖有Graphlab，numpy，jieba\n",
    "* Numpy和Jieba直接pip即可\n",
    "* Graphlab是商业软件，需要授权\n",
    "    * PIP安装时用此命令：(括号内可选)\n",
    "         * (sudo) pip install (--user) --upgrade --no-cache-dir https://get.graphlab.com/GraphLab-Create/2.0.1/ouyf5@mail2.sysu.edu.cn/F822-FCB4-973D-5C49-44E0-430C-BC81-4EA3/GraphLab-Create-License.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 首先是从文件中的读取以及分词\n",
    "\n",
    "\n",
    "此处用的是 *Jieba* 中文分词器，在这里我们将中文文本分词之后以空格分隔各词然后输出到目标文件夹\n",
    "\n",
    "函数定义中需传入 **CSV文件的绝对地址** \n",
    "\n",
    "格式要求：\n",
    "   * CSV中字串以UTF-8格式编码，CSV文件格式为id,content(需要有Header Row)\n",
    "   * 传入目标文件为绝对地址\n",
    "   * 获得的数据为SFrame格式，column与CSV格式一致\n",
    "   * SFrame参考文档：https://turi.com/products/create/docs/generated/graphlab.SFrame.html?highlight=sframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首先是添加停词表（Stop Words List）\n",
    "* 加入停词表能够把常用的词语（通常是介词、连接词等）去掉，提高分类的准确性\n",
    "* get_stop_words 这个方法从文件中读取停词列表，然后返回一个 **停词集合**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_from_file(path):\n",
    "    with open(path,\"r\") as fp:\n",
    "        words = fp.read()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stop_words(filepath):\n",
    "    words = read_from_file(filepath)\n",
    "    result = jieba.cut(words)\n",
    "    new_words = []\n",
    "    for r in result:\n",
    "        new_words.append(r)\n",
    "    return set(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(words,stop_words_set):\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if w not in stop_words_set:\n",
    "            new_words.append(w)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint: 检查上述方法的正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_and_cut' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e861df7afb14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#    print lister[i]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mweibo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_and_cut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m#print weibo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_and_cut' is not defined"
     ]
    }
   ],
   "source": [
    "#stop_set = get_stop_words('./stop.txt')\n",
    "#lister = list(stop_set)\n",
    "#for i in range(10):\n",
    "#    print lister[i]\n",
    "\n",
    "weibo = read_and_cut('./data.csv')\n",
    "#print weibo\n",
    "\n",
    "train_fake_news = weibo[0:10]\n",
    "train_fake_weibos = weibo[10:]\n",
    "\n",
    "weibos_wvec = batch_word_vec_generator(train_fake_weibos['parsed'] , True)\n",
    "news_wvec = batch_word_vec_generator(train_fake_news['parsed'] , True)\n",
    "\n",
    "\n",
    "train_fake_news['word_vec'] = news_wvec['word_vec']\n",
    "train_fake_news['tf_word_vec'] = news_wvec['tf_word_vec']\n",
    "train_fake_news['assign_weibos'] = batch_NN_finder(train_fake_news['tf_word_vec'] , weibos_wvec['tf_word_vec'] , train_fake_weibos['Id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是正式操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_and_cut(csvpath):\n",
    "    mydata = graphlab.SFrame(csvpath)\n",
    "    stop_set = get_stop_words('./stop.txt')\n",
    "    parsed = []\n",
    "    linen = {0,1,2,3,4,5,6,7,8,9,19,39,59,99,399,599,999,2999,3999,5999,8888,9999}\n",
    "    \n",
    "    for i in range(len(mydata)):\n",
    "        line = unicode(mydata['text'][i] , \"utf-8\")\n",
    "        seglist = jieba.cut(line , cut_all = False)\n",
    "        parsed_seg = remove_stop_words(seglist , stop_set)\n",
    "        output = ' '.join(parsed_seg)\n",
    "        parsed.append(output)\n",
    "        if (i in linen or i == len(mydata) - 1):\n",
    "            print \"处理 #%d 文章完成\" % i\n",
    "    mydata['parsed'] = parsed\n",
    "    print \"全部完成！\"\n",
    "    return mydata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来这个函数是用来提取*词数向量*的\n",
    "\n",
    "* 在分析过程中，可以频繁的调用该函数，需要提供 **原始文本的字串**\n",
    "* 返回值为一个 *Dict<单词，出现次数>* ,\n",
    "* 返回的Dict没有依据单词出现的次数排序\n",
    "* 如果用向量实现并不现实，所以在此处我们使用Dict来实现词数向量的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_word_vec(ref_str):\n",
    "    words = [w.strip() for w in ref_str.split() if w.strip()]\n",
    "    counter = Counter()\n",
    "    for w in words:\n",
    "        counter[w] += 1\n",
    "    # kv = counter.items()   #这是键值对的列表\n",
    "    wc_dict = dict(counter)    #这是词典\n",
    "    #可以用列表做其他事，但只返回词典\n",
    "    return wc_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 接下来是计算两个词数向量间的距离\n",
    "\n",
    "* euc_dist 计算两个词数向量之间的欧几里得距离 返回值为浮点数\n",
    "* cos_dist 计算两个词数向量之间的余弦距离 返回值为浮点数\n",
    "* jac_dist 计算两个词数向量之间的加权杰卡德距离 返回值为浮点数\n",
    "\n",
    "注意到，此处只是使用计算距离对词数向量进行Brute-Force计算，很容易把一些常见的词错误的认为是判断两个文本文档相类似的函数的主要依据（如 **\"的\"** , **\"和\"** , **\"是\"**  , **\"了\"**  , etc）。如果需要计算更精确的数值，应该使用 **TF-IDF** 来进行计算,使用者在使用前应该调用 *calc_tf_idf* 方法。总体来说，即便使用TFIDF处理过，中文文本的聚类正确率还是不是非常的理想。所以可以尽情尝试各种距离（经过初步实验，Euclidean Distance准确度稍微高那么一点点。）\n",
    "\n",
    "* 注意, *calc_tf_idf* 方法需要使用者传入所有文本（whole documents）的 **SArray词典集合（SArray of dict）** 返回值为SArray的词典集合类型（每行分别为原来单词的dict的SArray集合）。(SArray文档：https://turi.com/products/create/docs/generated/graphlab.SArray.html?highlight=sarray)\n",
    "* 示例使用方法：\n",
    "    ```python\n",
    "        docs['TF_IDF'] = calc_tf_idf(docs['wc_dict'])\n",
    "    ```\n",
    "    这样处理之后还是可以通过\n",
    "    ```python\n",
    "    docs[docs['dName'] == 'docnamehere'][0]\n",
    "    ```\n",
    "    找到某行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def euc_dist(dict_a , dict_b):\n",
    "    return graphlab.distances.euclidean(dict_a , dict_b)\n",
    "\n",
    "def cos_dist(dict_a , dict_b):\n",
    "    return graphlab.distances.cosine(dict_a , dict_b)\n",
    "\n",
    "def jac_dist(dict_a , dict_b):\n",
    "    return graphlab.distances.weighted_jaccard(dict_a , dict_b)\n",
    "\n",
    "def set_calculation(dict_a , dict_b):\n",
    "    seta = set(dict_a.keys())\n",
    "    setb = set(dict_b.keys())\n",
    "    inter = seta.intersection(setb)\n",
    "    result = 0\n",
    "    for val in inter:\n",
    "        result = result + dict_a[val] * dict_b[val]\n",
    "    return result\n",
    "\n",
    "# set similarity: Larger is better\n",
    "def set_dist(dict_small , dict_large , lg_whole_orig , lg_whole_vec):\n",
    "    result = 0\n",
    "    for k in dict_small:\n",
    "        if k in dict_large and dict_small[k] > 0:\n",
    "            # threshold: 0.1\n",
    "            if lg_whole_vec[k] <= 0.1 and dict_large[k] > 0:\n",
    "                result = lg_whole_orig[k] * dict_small[k] + result\n",
    "            elif dict_large[k] <= 0:\n",
    "                result = result + 0\n",
    "            else:\n",
    "                result = lg_whole_vec[k] * dict_small[k] + result\n",
    "        else:\n",
    "            result  = result + 0\n",
    "    return result\n",
    "\n",
    "def calc_tf_idf(whole_document_dict):\n",
    "    return graphlab.text_analytics.tf_idf(whole_document_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def whole_wc_vec(whole_dict):\n",
    "    wholes = {}\n",
    "    for i in range(len(whole_dict)):\n",
    "        test = whole_dict[i]\n",
    "        for k in test:\n",
    "            if k not in wholes:\n",
    "                wholes[k] = test[k]\n",
    "            else:\n",
    "                wholes[k] = wholes[k] + test[k]\n",
    "    return wholes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 给某一个特定的新闻找到10个左右Nearnest Neighbour\n",
    "\n",
    "由于我们在数据库中存放的是微博，所以我们简单的给定 *新闻文档* ，然后给其适配最适合的10个微博。\n",
    "* 距离此处使用了欧式距离。\n",
    "* 初始阈值设定为50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 不管传入的是否是TF-IDF，要求传的新闻词数向量为Dictionary，whole_document_vec为dict的SFrame\n",
    "def find_NN(news_vec , whole_document_vec , doc_ids):\n",
    "    process_vec = graphlab.SFrame()\n",
    "    #process_vec['original'] = whole_document_vec\n",
    "    result_value = []\n",
    "    result_id = []\n",
    "\n",
    "    for i in range(len(whole_document_vec)):\n",
    "        cos_val = set_calculation(whole_document_vec[i] , news_vec)\n",
    "        if cos_val <= 10000:\n",
    "            result_id.append(doc_ids[i])\n",
    "            result_value.append(cos_val)\n",
    "    process_vec['similarity'] = result_value\n",
    "    process_vec['id'] = result_id\n",
    "\n",
    "    my = process_vec.sort('similarity' , ascending=False)\n",
    "\n",
    "    if len(process_vec) < 10:\n",
    "        return my\n",
    "    else:\n",
    "        return my[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量生成工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_word_vec_generator(dict_set , TF_SELECTOR = False):\n",
    "        gen_result = graphlab.SFrame()\n",
    "        word_vec = []\n",
    "        linen = {0,1,2,3,4,5,6,7,8,9,19,39,59,99,399,599,999,2999,3999,5999,8888,9999}\n",
    "        for i in range(len(dict_set)):\n",
    "                temp_vec = parse_word_vec(dict_set[i])\n",
    "                word_vec.append(temp_vec)\n",
    "                if i in linen or i == len(dict_set) - 1:\n",
    "                    print \"第 %d 条文章原始词向量计算完成\" % i\n",
    "        gen_result['word_vec'] = word_vec\n",
    "        print \"本数据集的原始词向量全部计算完成\"\n",
    "        if TF_SELECTOR:\n",
    "                gen_result['tf_word_vec'] = calc_tf_idf(gen_result['word_vec'])\n",
    "                print \"本数据集的TF-IDF词向量全部计算完成\"\n",
    "        return gen_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_NN_finder(news , weibos , weibo_ids):\n",
    "    linen = {0,1,2,3,4,5,6,7,8,9,19,39,59,99,199,399,599,799,999,1299,1499,1699,1999,2499,2999,3499,3999,4499,4999,5999,8888,9999}\n",
    "    assignation = []\n",
    "    for i in range(len(news)):\n",
    "        temp_nns = find_NN(news[i] , weibos , weibo_ids)\n",
    "        assignation.append(temp_nns)\n",
    "        if i in linen or i == len(news) - 1:\n",
    "            print \"第 %d 条新闻的最近邻居计算完成\" % (i + 1)\n",
    "    return assignation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数测试完成，下面是预处理总方法\n",
    "\n",
    "由于我们需要的是新闻AND ITS 匹配微博\n",
    "所以本方法只返回一个SFrame，列为 <*新闻ID* , *新闻原文* , *新闻原始词向量* ,*tf词向量* ,*匹配微博* >\n",
    "\n",
    "注意：**匹配微博**为一个对象，其中含有匹配微博的< *id* , *相似度*>\n",
    "\n",
    "\n",
    "接受的参数为：\n",
    "* weibo_src: 微博CSV文件的绝对路径\n",
    "* news_src: 新闻CSV文件的绝对路径\n",
    "* TF_SELECTOR: TF-IDF选择子，表示是否需要进行TF词向量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Main process\n",
    "#Result contains a SFrame that contains news raw word_vec and matching weibo\n",
    "def pre_process(weibo_src , news_src , TF_SELECTOR=False):\n",
    "    #分别处理微博和新闻\n",
    "    weibos = read_and_cut(weibo_src)\n",
    "    news = read_and_cut(news_src)\n",
    "    \n",
    "    #计算词向量和TF词向量\n",
    "    weibos_wvec = batch_word_vec_generator(weibos['parsed'] , TF_SELECTOR)\n",
    "    news_wvec = batch_word_vec_generator(news['parsed'] , TF_SELECTOR)\n",
    "    \n",
    "    news['word_vec'] = news_wvec['word_vec']\n",
    "    \n",
    "    if TF_SELECTOR:\n",
    "        news['tf_word_vec'] = news_wvec['tf_word_vec']\n",
    "        news['assign_weibos'] = batch_NN_finder(news['tf_word_vec'] , weibos_wvec['tf_word_vec'] , weibos['id'])\n",
    "    else:\n",
    "        news['assign_weibos'] = batch_NN_finder(news['word_vec'] , weibos_wvec['word_vec'] , weibos['id'])\n",
    "\n",
    "    return news,weibos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create is assigned to ouyf5@mail2.sysu.edu.cn and will expire on November 25, 2016. For commercial licensing options, visit https://turi.com/buy/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-07-21 17:45:48,424 [INFO] graphlab.cython.cy_server, 176: GraphLab Create v1.8.5 started. Logging: /tmp/graphlab_server_1469094345.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5JkIGf\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5JkIGf\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5lVabK\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5lVabK\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55WxAd\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55WxAd\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55YTvx\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55YTvx\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"你的委屈要自己消化 你的故事不用逢人就讲起\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"你的委屈要自己消化 你的故事不用逢人就讲起\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"真正理解你的没有几个\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"真正理解你的没有几个\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55uIb1\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55uIb1\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"大多人只会站在他们自己的立场 偷看你的笑话\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"大多人只会站在他们自己的立场 偷看你的笑话\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"你能做的就是 把秘密藏起来 然后一步一步变得越来越强大”（via思想聚焦）\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"你能做的就是 把秘密藏起来 然后一步一步变得越来越强大”（via思想聚焦）\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5j1pM0\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5j1pM0\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>52 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "52 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/weibo.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/weibo.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.053062 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.053062 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[int,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5JkIGf\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5JkIGf\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55WxAd\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55WxAd\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55YTvx\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55YTvx\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R55uIb1\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R55uIb1\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5tE6jp\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5tE6jp\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"http://t.cn/R5lVabK\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"http://t.cn/R5lVabK\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"周到上海致力于为上海市民打造一个生活服务指南平台，让你足不出户知晓申城大小事，动动手指解决身边烦心事。\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"周到上海致力于为上海市民打造一个生活服务指南平台，让你足不出户知晓申城大小事，动动手指解决身边烦心事。\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"——路遥《平凡的世界》\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"——路遥《平凡的世界》\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"你的委屈要自己消化 你的故事不用逢人就讲起\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"你的委屈要自己消化 你的故事不用逢人就讲起\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"真正理解你的没有几个\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"真正理解你的没有几个\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>52 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "52 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-07-21 17:45:48,610 [DEBUG] jieba, 111: Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "2016-07-21 17:45:48,886 [DEBUG] jieba, 131: Loading model from cache /tmp/jieba.cache\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/weibo.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/weibo.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.523 seconds.\n",
      "2016-07-21 17:45:49,409 [DEBUG] jieba, 163: Loading model cost 0.523 seconds.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 4379 lines in 0.055248 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 4379 lines in 0.055248 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built succesfully.\n",
      "2016-07-21 17:45:49,414 [DEBUG] jieba, 164: Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理 #0 文章完成\n",
      "处理 #1 文章完成\n",
      "处理 #2 文章完成\n",
      "处理 #3 文章完成\n",
      "处理 #4 文章完成\n",
      "处理 #5 文章完成\n",
      "处理 #6 文章完成\n",
      "处理 #7 文章完成\n",
      "处理 #8 文章完成\n",
      "处理 #9 文章完成\n",
      "处理 #19 文章完成\n",
      "处理 #39 文章完成\n",
      "处理 #59 文章完成\n",
      "处理 #99 文章完成\n",
      "处理 #399 文章完成\n",
      "处理 #599 文章完成\n",
      "处理 #999 文章完成\n",
      "处理 #2999 文章完成\n",
      "处理 #3999 文章完成\n",
      "处理 #4378 文章完成\n",
      "全部完成！\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t作者 宋宁\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t作者 宋宁\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t　2016-06-12 11:25\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t　2016-06-12 11:25\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t交汇点讯 6月12日早上7点多，南京雨下个不停，在江宁区东山街道大里聚福城的康居园1栋，有人发现一名老太坠楼，大家赶 紧报警。警察法医赶到现场，发现老太已经死亡。老太坠楼地点位于...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t交汇点讯 6月12日早上7点多，南京雨下个不停，在江宁区东山街道大里聚福城的康居园1栋，有人发现一名老太坠楼，大家赶 紧报警。警察法医赶到现场，发现老太已经死亡。老太坠楼地点位于...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t“有意见，你去咬狗一口啊”■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t“有意见，你去咬狗一口啊”■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        粤公网安备 44010402000035号\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        粤公网安备 44010402000035号\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>183 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "183 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/news.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/news.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.086001 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.086001 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first line of file as \n",
      "column_type_hints=[str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t作者 宋宁\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t作者 宋宁\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t　2016-06-12 11:25\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t　2016-06-12 11:25\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t交汇点讯 6月12日早上7点多，南京雨下个不停，在江宁区东山街道大里聚福城的康居园1栋，有人发现一名老太坠楼，大家赶 紧报警。警察法医赶到现场，发现老太已经死亡。老太坠楼地点位于...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t交汇点讯 6月12日早上7点多，南京雨下个不停，在江宁区东山街道大里聚福城的康居园1栋，有人发现一名老太坠楼，大家赶 紧报警。警察法医赶到现场，发现老太已经死亡。老太坠楼地点位于...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t“有意见，你去咬狗一口啊”■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t“有意见，你去咬狗一口啊”■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"        主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖北日报社） 主管：中共湖北省委宣传部 湖北省人民政府新闻办公室　主办：湖北日报传媒集团（湖�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\t\t\t■记者 陈海东　　通讯员 对湖综　　福州晚报讯突然被路边冲出的一条狗咬伤，女生要求在场的狗主人赔偿。不想对方竟称“如果有意见，你自己去咬狗一口啊”。　　6月22日傍晚，女生小�...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"渝ICP备12004790号-1 公安部门备案编号：5000085100200005  ©2015上游\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"渝ICP备12004790号-1 公安部门备案编号：5000085100200005  ©2015上游\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>183 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "183 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/news.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /home/souler/ml-wst/classification/Information Retrieval/news.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1493 lines in 0.12073 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1493 lines in 0.12073 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理 #0 文章完成\n",
      "处理 #1 文章完成\n",
      "处理 #2 文章完成\n",
      "处理 #3 文章完成\n",
      "处理 #4 文章完成\n",
      "处理 #5 文章完成\n",
      "处理 #6 文章完成\n",
      "处理 #7 文章完成\n",
      "处理 #8 文章完成\n",
      "处理 #9 文章完成\n",
      "处理 #19 文章完成\n",
      "处理 #39 文章完成\n",
      "处理 #59 文章完成\n",
      "处理 #99 文章完成\n",
      "处理 #399 文章完成\n",
      "处理 #599 文章完成\n",
      "处理 #999 文章完成\n",
      "处理 #1492 文章完成\n",
      "全部完成！\n"
     ]
    }
   ],
   "source": [
    "weibopath = './weibo.csv'\n",
    "newspath = './news.csv'\n",
    "TF_SELECTOR = True\n",
    "\n",
    "weibos = read_and_cut(weibopath)\n",
    "news = read_and_cut(newspath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算微博词向量\n",
      "第 0 条文章原始词向量计算完成\n",
      "第 1 条文章原始词向量计算完成\n",
      "第 2 条文章原始词向量计算完成\n",
      "第 3 条文章原始词向量计算完成\n",
      "第 4 条文章原始词向量计算完成\n",
      "第 5 条文章原始词向量计算完成\n",
      "第 6 条文章原始词向量计算完成\n",
      "第 7 条文章原始词向量计算完成\n",
      "第 8 条文章原始词向量计算完成\n",
      "第 9 条文章原始词向量计算完成\n",
      "第 19 条文章原始词向量计算完成\n",
      "第 39 条文章原始词向量计算完成\n",
      "第 59 条文章原始词向量计算完成\n",
      "第 99 条文章原始词向量计算完成\n",
      "第 399 条文章原始词向量计算完成\n",
      "第 599 条文章原始词向量计算完成\n",
      "第 999 条文章原始词向量计算完成\n",
      "第 2999 条文章原始词向量计算完成\n",
      "第 3999 条文章原始词向量计算完成\n",
      "第 4378 条文章原始词向量计算完成\n",
      "本数据集的原始词向量全部计算完成\n",
      "本数据集的TF-IDF词向量全部计算完成\n",
      "微博词向量全部计算完成\n",
      "\n",
      "开始计算新闻词向量\n",
      "第 0 条文章原始词向量计算完成\n",
      "第 1 条文章原始词向量计算完成\n",
      "第 2 条文章原始词向量计算完成\n",
      "第 3 条文章原始词向量计算完成\n",
      "第 4 条文章原始词向量计算完成\n",
      "第 5 条文章原始词向量计算完成\n",
      "第 6 条文章原始词向量计算完成\n",
      "第 7 条文章原始词向量计算完成\n",
      "第 8 条文章原始词向量计算完成\n",
      "第 9 条文章原始词向量计算完成\n",
      "第 19 条文章原始词向量计算完成\n",
      "第 39 条文章原始词向量计算完成\n",
      "第 59 条文章原始词向量计算完成\n",
      "第 99 条文章原始词向量计算完成\n",
      "第 399 条文章原始词向量计算完成\n",
      "第 599 条文章原始词向量计算完成\n",
      "第 999 条文章原始词向量计算完成\n",
      "第 1492 条文章原始词向量计算完成\n",
      "本数据集的原始词向量全部计算完成\n",
      "本数据集的TF-IDF词向量全部计算完成\n",
      "新闻词向量全部计算完成\n"
     ]
    }
   ],
   "source": [
    "print \"开始计算微博词向量\"\n",
    "weibos_wvec = batch_word_vec_generator(weibos['parsed'] , TF_SELECTOR)\n",
    "print \"微博词向量全部计算完成\"\n",
    "print \"\"\n",
    "print \"开始计算新闻词向量\"\n",
    "news_wvec = batch_word_vec_generator(news['parsed'] , TF_SELECTOR)\n",
    "print \"新闻词向量全部计算完成\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算最近邻居\n",
      "第 1 条新闻的最近邻居计算完成\n",
      "第 2 条新闻的最近邻居计算完成\n",
      "第 3 条新闻的最近邻居计算完成\n",
      "第 4 条新闻的最近邻居计算完成\n",
      "第 5 条新闻的最近邻居计算完成\n",
      "第 6 条新闻的最近邻居计算完成\n",
      "第 7 条新闻的最近邻居计算完成\n",
      "第 8 条新闻的最近邻居计算完成\n",
      "第 9 条新闻的最近邻居计算完成\n",
      "第 10 条新闻的最近邻居计算完成\n",
      "第 20 条新闻的最近邻居计算完成\n",
      "第 40 条新闻的最近邻居计算完成\n",
      "第 60 条新闻的最近邻居计算完成\n",
      "第 100 条新闻的最近邻居计算完成\n",
      "第 200 条新闻的最近邻居计算完成\n",
      "第 400 条新闻的最近邻居计算完成\n",
      "第 600 条新闻的最近邻居计算完成\n",
      "第 800 条新闻的最近邻居计算完成\n",
      "第 1000 条新闻的最近邻居计算完成\n",
      "第 1300 条新闻的最近邻居计算完成\n",
      "第 1493 条新闻的最近邻居计算完成\n",
      "最近邻居全部计算完成\n"
     ]
    }
   ],
   "source": [
    "news['word_vec'] = news_wvec['word_vec']\n",
    "news['tf_word_vec'] = news_wvec['tf_word_vec']\n",
    "print \"开始计算最近邻居\"\n",
    "\n",
    "\n",
    "news['assign_weibos'] = batch_NN_finder(news['tf_word_vec'] , weibos_wvec['tf_word_vec'] , weibos['id'])\n",
    "print \"最近邻居全部计算完成\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news.save('./news_finished')\n",
    "weibos.save('./weibo_parsed')\n",
    "weibos_wvec.save('./weibo_wordvec')\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(test)):\n",
    "#     print \"\"\n",
    "#     print \"+++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "#     print \"-------------------------------------------------------\"\n",
    "#     print \"分析新闻编号： %s.\" % test['id'][i]\n",
    "#     print \"该新闻内容如下：\"\n",
    "#     print test['text'][i]\n",
    "#     print \"\"\n",
    "#     print \"总共匹配文章数： %d.\" % len(test['assign_weibos'][i])\n",
    "#     print \"以下是匹配列表：\"\n",
    "#     print \"-------------------------------------------------------\"\n",
    "#     for ii in range(len(test['assign_weibos'][i])):\n",
    "#         print \"--------------------------------------------------------\"\n",
    "#         print \"匹配微博： #%d.\" % test['assign_weibos'][i][ii]['id']\n",
    "#         print weibos[weibos['id'] == test['assign_weibos'][i][ii]['id']]['text']\n",
    "#         print \"计算所得距离系数： %f.\" % test['assign_weibos'][i][ii]['similarity']\n",
    "#         print \"--------------------------------------------------------\"\n",
    "#     print \"--------------------------------------------------------\"\n",
    "#     print \"++++++++++++++++++++++++++++++++++++++++++++++++++++++++\"\n",
    "#     print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始计算微博总词向量\n",
      "开始计算新闻总词向量\n"
     ]
    }
   ],
   "source": [
    "# print \"开始计算微博总词向量\"\n",
    "# whole_weibos_dict = whole_wc_vec(weibos_wvec['tf_word_vec'])\n",
    "# print \"开始计算新闻总词向量\"\n",
    "# whole_news_dict = whole_wc_vec(news['tf_word_vec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846.25709595\n",
      "4093.24159478\n"
     ]
    }
   ],
   "source": [
    "# #len(whole_weibos_dict)\n",
    "# x = whole_weibos_dict.values()\n",
    "# x.sort(reverse=True)\n",
    "# print x[0]\n",
    "\n",
    "# y = whole_news_dict.values()\n",
    "# y.sort(reverse=True)\n",
    "# print y[0]\n",
    "\n",
    "# weibo_reg = dict(whole_weibos_dict)\n",
    "# for k in weibo_reg:\n",
    "#     weibo_reg[k] = weibo_reg[k] / x[0]\n",
    "\n",
    "# news_reg = dict(whole_news_dict)\n",
    "# for k in news_reg:\n",
    "#     news_reg[k] = news_reg[k] / y[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 这样，重启的时候只需要\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graphlab.load_sframe('mysframedir') \n",
    "\n",
    "\n",
    "news = graphlab.load_sframe('./news_finished')\n",
    "weibos = graphlab.load_sframe('./weibo_parsed')\n",
    "weibos_wvec = graphlab.load_sframe('./weibo_wordvec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------------------------+\n",
      "| id |                           text                          |\n",
      "+----+---------------------------------------------------------+\n",
      "| 1  |   　　还有两天，2016全国高考就要开始了！　　就在考...   |\n",
      "| 5  |  从昨天（3日）开始，在上海，有一群人冒着雨，在上海影... |\n",
      "| 6  |    　　新华社无锡6月4日体育专电(记者 王镜宇 王恒志...   |\n",
      "| 9  |                    在郑州经营近3年的方大同胡辣...       |\n",
      "| 10 | 　　今天要说的这个故事，也许只有在电视剧里才会看到。... |\n",
      "| 12 |  6月3日，路透社报道称，有据知情人士表示，捷豹路虎正...  |\n",
      "| 14 | 　　坐飞机的时候，会不会总是担心飞机的安全性？最近乌... |\n",
      "| 17 |   　　原标题：“史上最严”高考将临 多地部署严防大学生...  |\n",
      "| 18 | 毛坦厂是安徽的一座僻静小镇，周围是沟壑丛生的山峦。可... |\n",
      "| 20 | 　　今天要说的这个故事，也许只有在电视剧里才会看到。... |\n",
      "+----+---------------------------------------------------------+\n",
      "+--------------------------------------------------+\n",
      "|                      parsed                      |\n",
      "+--------------------------------------------------+\n",
      "|   两天 2016 全国 高考 考生 抓紧 复习 冲刺 ...    |\n",
      "|  昨天 上海 一群 雨 上海 影院 门口 等待 今早 ...  |\n",
      "|   新华社 无锡 月 体育 专电 记者   王镜宇   ...   |\n",
      "|                                 郑州...          |\n",
      "|  要说 故事 也许 电视剧 里 找到 一对 夫妻 听 ...  |\n",
      "|  月 路透社 报道 称 有据 知情 人士 捷豹 路 虎...  |\n",
      "| 坐飞机 担心 飞机 安全性 乌克兰 一位 工程师 提... |\n",
      "|  原 标题 史上 最严 高考 将临   多地 部署 严...   |\n",
      "| 毛坦厂 安徽 一座 僻静 小镇 沟壑 丛生 山峦 这...  |\n",
      "|  要说 故事 也许 电视剧 里 找到 一对 夫妻 听 ...  |\n",
      "+--------------------------------------------------+\n",
      "+-------------------------------+-------------------------------+\n",
      "|            word_vec           |          tf_word_vec          |\n",
      "+-------------------------------+-------------------------------+\n",
      "| {'\\xe5\\x8f\\x91\\xe7\\x8e\\xb0... | {'\\xe5\\x8f\\x91\\xe7\\x8e\\xb0... |\n",
      "| {'\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9... | {'\\xe4\\xbb\\x8a\\xe5\\xa4\\xa9... |\n",
      "| {'\\xe6\\x9c\\x88': 2, '\\xe7\\... | {'\\xe6\\x9c\\x88': 0.9062679... |\n",
      "| {'\\xe5\\x81\\x9a\\xe6\\xa2\\xa6... | {'\\xe5\\x81\\x9a\\xe6\\xa2\\xa6... |\n",
      "| {'\\xe5\\xa4\\x9a\\xe5\\x9b\\xbd... | {'\\xe5\\xa4\\x9a\\xe5\\x9b\\xbd... |\n",
      "| {'\\xe5\\x8f\\x91\\xe7\\x8e\\xb0... | {'\\xe5\\x8f\\x91\\xe7\\x8e\\xb0... |\n",
      "| {'\\xe5\\xb7\\xa5\\xe7\\xa8\\x8b... | {'\\xe5\\xb7\\xa5\\xe7\\xa8\\x8b... |\n",
      "| {'\\xe6\\x95\\x99\\xe8\\x82\\xb2... | {'\\xe6\\x95\\x99\\xe8\\x82\\xb2... |\n",
      "| {'\\xe8\\xbe\\xbe\\xe7\\xba\\xbf... | {'\\xe8\\xbe\\xbe\\xe7\\xba\\xbf... |\n",
      "| {'\\xe7\\xab\\x99': 1, '\\xe6\\... | {'\\xe7\\xab\\x99': 2.8541955... |\n",
      "+-------------------------------+-------------------------------+\n",
      "+-------------------------------+\n",
      "|         assign_weibos         |\n",
      "+-------------------------------+\n",
      "| [{'id': 3982762123284430, ... |\n",
      "| [{'id': 3985335827986880, ... |\n",
      "| [{'id': 3983279083827168, ... |\n",
      "| [{'id': 3991994008741733, ... |\n",
      "| [{'id': 3982917023195748, ... |\n",
      "| [{'id': 3982784882238474, ... |\n",
      "| [{'id': 3982792544493101, ... |\n",
      "| [{'id': 3982896198465899, ... |\n",
      "| [{'id': 3983002100422049, ... |\n",
      "| [{'id': 3982917023195748, ... |\n",
      "+-------------------------------+\n",
      "[1493 rows x 6 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "print news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------------------------------------------+\n",
      "|        id        |                          text                          |\n",
      "+------------------+--------------------------------------------------------+\n",
      "| 3982760672611680 |  【四川广元一艘客轮发生翻沉 船上18人4人已获救】今...   |\n",
      "| 3982762123284430 | 【还有两天了！请收藏这份高考备忘录！】还有两天，20...  |\n",
      "| 3982765202715016 |  #轻松一刻#网友@爱玩的欧尼们 剪辑了基情版《太阳的...   |\n",
      "| 3982765705404585 |  【男子白天醉驾摩托被查 怪民警“不按常规出牌”】5月...   |\n",
      "| 3982765705883442 | 【有能力的策划大牛，别忘了周刊君在等你[挤眼]】你需...  |\n",
      "| 3982765810070058 | 【正直播！央视记者深入洪水“吞没”的法国小镇】连日暴...  |\n",
      "| 3982767467794365 | 猴年马月就要来了，你的当初梦想快要实现了吗？“如果你... |\n",
      "| 3982768222299077 | 【#最养生的走路法#：竟有意想不到的效果！】走路是世...  |\n",
      "| 3982770327468562 |   #热点#【柯洁年内将战“阿尔法狗” 你看好谁赢？】国...   |\n",
      "| 3982771523437877 |  【今早好多人定闹钟准备抢TA！还有人通宵排队19个小...   |\n",
      "+------------------+--------------------------------------------------------+\n",
      "+-------------------------------------------------+\n",
      "|                      parsed                     |\n",
      "+-------------------------------------------------+\n",
      "|   四川 广元 一艘 客轮 发生 翻沉   船上 18 ...   |\n",
      "|   两天 请 收藏 这份 高考 备忘录 两天 2016 ...   |\n",
      "|  轻松 一刻 网友 爱玩 欧尼们   剪辑 基情 版 ...  |\n",
      "|  男子 白天 醉驾 摩托 被查   民警 常规 牌 月...  |\n",
      "| 能力 策划 大牛 别忘了 周刊 君 挤眼 策划 创意... |\n",
      "|  正 直播 央视 记者 洪水 吞没 法国 小镇 暴雨 ... |\n",
      "|     猴年马月 当初 梦想 梦想 捍卫 心 晚安...     |\n",
      "| 养生 走路 法 竟有 意想不到 效果 走路 世界 最... |\n",
      "|  热点 柯洁 年内 将战 阿尔法 狗   看好 赢 国...  |\n",
      "|    今早 好多 闹钟 抢 TA 通宵 排队 19 小时 ...   |\n",
      "+-------------------------------------------------+\n",
      "[4379 rows x 3 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "print weibos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------+\n",
      "|            word_vec           |          tf_word_vec          |\n",
      "+-------------------------------+-------------------------------+\n",
      "| {'\\xe5\\x8f\\x91\\xe7\\x94\\x9f... | {'\\xe5\\x8f\\x91\\xe7\\x94\\x9f... |\n",
      "| {'cn': 1, '\\xe5\\x8c\\x85\\xe... | {'cn': 0.5450500850967199,... |\n",
      "| {'cn': 1, '\\xe9\\x98\\xb4\\xe... | {'cn': 0.5450500850967199,... |\n",
      "| {'\\xe5\\x8f\\x91\\xe7\\x8e\\xb0... | {'\\xe5\\x8f\\x91\\xe7\\x8e\\xb0... |\n",
      "| {'\\xe4\\xba\\xba\\xe7\\x89\\xa9... | {'\\xe4\\xba\\xba\\xe7\\x89\\xa9... |\n",
      "| {'\\xe5\\xbd\\xb1\\xe5\\x93\\x8d... | {'\\xe5\\xbd\\xb1\\xe5\\x93\\x8d... |\n",
      "| {'\\xe7\\x8c\\xb4\\xe5\\xb9\\xb4... | {'\\xe7\\x8c\\xb4\\xe5\\xb9\\xb4... |\n",
      "| {'\\xe7\\xa9\\xbf\\xe7\\x9d\\x80... | {'\\xe7\\xa9\\xbf\\xe7\\x9d\\x80... |\n",
      "| {'\\xe6\\x9c\\x88': 1, 'cn': ... | {'\\xe6\\x9c\\x88': 1.7847051... |\n",
      "| {'\\xe9\\x97\\xb9\\xe9\\x92\\x9f... | {'\\xe9\\x97\\xb9\\xe9\\x92\\x9f... |\n",
      "+-------------------------------+-------------------------------+\n",
      "[4379 rows x 2 columns]\n",
      "Note: Only the head of the SFrame is printed.\n",
      "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n"
     ]
    }
   ],
   "source": [
    "print weibos_wvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面是各种数据的运算：\n",
    "* 首先是算二词的出现次数bi-gram occurence 输入的为**分割好的（没有去除任何词语的）文本**，返回*二词词典*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "获救送往 出现 1 次\n",
      "发生翻沉 出现 1 次\n",
      "已有获救 出现 1 次\n",
      "获救14 出现 1 次\n",
      "四川广元 出现 2 次\n",
      "一艘客轮 出现 1 次\n",
      "分四川 出现 1 次\n",
      "50分 出现 1 次\n",
      "央视新闻 出现 1 次\n",
      "40双龙 出现 1 次\n",
      "四川日报 出现 1 次\n",
      "准载40 出现 1 次\n",
      "出事船上 出现 1 次\n",
      "送往医院 出现 1 次\n",
      "医院救治 出现 1 次\n",
      "广元龙湖 出现 1 次\n",
      "侧翻出事 出现 1 次\n",
      "客轮发生 出现 1 次\n",
      "船上18 出现 2 次\n",
      "船上工作人员 出现 1 次\n",
      "18获救 出现 1 次\n",
      "新闻四川 出现 1 次\n",
      "号侧翻 出现 1 次\n",
      "龙湖张家湾 出现 1 次\n",
      "营救已有 出现 1 次\n",
      "另有船上 出现 1 次\n",
      "张家湾湖面 出现 1 次\n",
      "消防营救 出现 1 次\n",
      "翻沉船上 出现 1 次\n",
      "获救获救 出现 1 次\n",
      "18人中 出现 1 次\n",
      "救治央视 出现 1 次\n",
      "湖面突发 出现 1 次\n",
      "大风准载 出现 1 次\n",
      "双龙号 出现 1 次\n",
      "广元一艘 出现 1 次\n",
      "突发大风 出现 1 次\n",
      "人中16 出现 1 次\n",
      "1450 出现 1 次\n",
      "16人为 出现 1 次\n",
      "人为乘客 出现 1 次\n",
      "乘客另有 出现 1 次\n",
      "工作人员消防 出现 1 次\n"
     ]
    }
   ],
   "source": [
    "def cal_bigram(parsed_text):\n",
    "    words = [w.strip() for w in parsed_text.split() if w.strip()]\n",
    "    counter = Counter()\n",
    "    for i in range(len(words) - 1):\n",
    "        w = words[i] + words[i + 1]\n",
    "        counter[w] = counter[w] + 1\n",
    "    bidict = dict(counter)\n",
    "    return bidict\n",
    "mydict = cal_bigram(weibos['parsed'][0])\n",
    "for k in mydict:\n",
    "    print \"%s 出现 %d 次\" % (k , mydict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#自动分句方法，返回句数组\n",
    "def split_by_set(text):\n",
    "    return re.split('。|！' , text)\n",
    "#split_by_set(news['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 然后就是计算Wb,weibo(i) = 1/ *所有二词出现次数的和*，作为计算常数\n",
    "* 需要传入**weibo的二词字典**（上面计算给出）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_const(weibo_bidict):\n",
    "    return np.sum(weibo_bidict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
